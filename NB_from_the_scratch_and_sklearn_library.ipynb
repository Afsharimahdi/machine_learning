{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**NB from the scratch**\n",
        "\n",
        "\n",
        "This code defines a class called `GaussianNaiveBayes`, which implements Gaussian Naive Bayes classification. Here's a breakdown of the code's structure and functionality:\n",
        "\n",
        "1. The code starts with importing the necessary dependencies, specifically the `numpy` library for mathematical operations.\n",
        "\n",
        "2. The `GaussianNaiveBayes` class is defined.\n",
        "\n",
        "3. The `fit` method is responsible for training the Naive Bayes classifier. It takes two parameters: `X`, the input features, and `y`, the corresponding class labels. The method calculates the mean, variance, and prior probability for each class.\n",
        "\n",
        "4. Inside the `fit` method, the number of samples (`n_samples`) and number of features (`n_features`) are extracted from the input data `X`. The unique class labels (`self._classes`) are determined using `np.unique(y)`, and the number of classes (`n_classes`) is computed.\n",
        "\n",
        "5. Empty arrays for storing the mean (`self._mean`), variance (`self._var`), and prior probabilities (`self._priors`) are initialized with zeros.\n",
        "\n",
        "6. The code then enters a loop to calculate the mean, variance, and prior probability for each class. It iterates over the unique class labels using `enumerate(self._classes)`.\n",
        "\n",
        "7. Inside the loop, `X_for_class_c` is assigned the subset of data `X` that belongs to the current class `c`. The mean of each feature is computed using `X_for_class_c.mean(axis=0)` and stored in the `self._mean` array at the corresponding class index `i`. The variance of each feature is computed using `X_for_class_c.var(axis="
      ],
      "metadata": {
        "id": "F8I1VVapTFWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "```python\n",
        "\n",
        "This code defines a `GaussianNaiveBayes` class that implements\n",
        "Gaussian Naive Bayes classification algorithm.\n",
        " Here 's' a breakdown of what each section of the code does:\n",
        "\n",
        "- Lines 1-3: Importing the necessary libraries (`numpy`).\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "- Lines 5-37: Definition of the `GaussianNaiveBayes` class and its methods.\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "\n",
        "  - `fit` method:\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "\n",
        "        - Line 8: Receives the training data `X` and corresponding labels `y`.\n",
        "\n",
        "        n_classes = len(self._classes)\n",
        "\n",
        "        - Line 9: Computes the number of samples and number of features in the data.\n",
        "\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "\n",
        "        - Line 10: Determines the unique classes in the labels.\n",
        "\n",
        "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "\n",
        "          - Line 11: Calculates the number of classes.\n",
        "\n",
        "\n",
        "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "\n",
        "          - Lines 12-14: Initializes arrays to store the mean, variance, and priors for each class.\n",
        "\n",
        "\n",
        "        # calculating the mean, variance, and prior P(H) for each class\n",
        "        for i, c in enumerate(self._classes):\n",
        "            X_for_class_c = X[y==c]\n",
        "            self._mean[i, :] = X_for_class_c.mean(axis=0)\n",
        "            self._var[i, :] = X_for_class_c.var(axis=0)\n",
        "            self._priors[i] = X_for_class_c.shape[0] / float(n_samples)\n",
        "\n",
        "\n",
        "  - Lines 16-21: Iterates over each class and calculates the mean, variance, and prior probabilities by extracting the relevant subset of data for that class.\n",
        "\n",
        "\n",
        "    def _calculate_likelihood(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "\n",
        "        - Line 23: Calculates the likelihood of a feature value given a class by using the Gaussian probability density function.\n",
        "\n",
        "        num = np.exp(- (x - mean)**2 / (2 * var))  # numerator\n",
        "        denom = np.sqrt(2 * np.pi * var)  # denominator\n",
        "        return num / denom\n",
        "\n",
        "\n",
        "    - Line 26: Calculates the likelihoods for all features and returns the result.\n",
        "\n",
        "    - `predict` method:\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._classify_sample(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    - Line 30: Receives the test data `X`.\n",
        "\n",
        "    - Line 31: Calls the `_classify_sample` method for each sample in `X` and stores the predictions.\n",
        "\n",
        "\n",
        "  - `_classify_sample` method:\n",
        "\n",
        "    def _classify_sample(self, x):\n",
        "\n",
        "       - Line 32: Returns the predictions as a NumPy array.\n",
        "\n",
        "        posteriors = []\n",
        "\n",
        "        # calculating posterior probability for each class\n",
        "        for i, c in enumerate(self._classes):\n",
        "            prior = np.log(self._priors[i])\n",
        "\n",
        "            - Line 36: Receives a sample `x` to classify.\n",
        "\n",
        "            posterior = np.sum(np.log(self._calculate_likelihood(i, x)))\n",
        "\n",
        "            - Line 37: Initializes an empty list to store the posterior probabilities for each class.\n",
        "\n",
        "            posterior = prior + posterior\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "\n",
        "        - Lines 39-47: Iterates over each\n",
        "        # return the class with the highest posterior probability\n",
        "        return self._classes[np.argmax(posteriors)]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m5kzNYgpOAap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ri3RiH-8N-mT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._priors =  np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        # calculating the mean, variance and prior P(H) for each class\n",
        "        for i, c in enumerate(self._classes):\n",
        "            X_for_class_c = X[y==c]\n",
        "            self._mean[i, :] = X_for_class_c.mean(axis=0)\n",
        "            self._var[i, :] = X_for_class_c.var(axis=0)\n",
        "            self._priors[i] = X_for_class_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def _calculate_likelihood(self, class_idx, x):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = self._var[class_idx]\n",
        "        num = np.exp(- (x-mean)**2 / (2 * var)) #numerator\n",
        "        denom = np.sqrt(2 * np.pi * var) #denominator\n",
        "        return num / denom\n",
        "\n",
        "    def predict(self, X):\n",
        "         y_pred = [self._classify_sample(x) for x in X]\n",
        "         return np.array(y_pred)\n",
        "\n",
        "    def _classify_sample(self, x):\n",
        "         posteriors = []\n",
        "         # calculating posterior probability for each class\n",
        "         for i, c in enumerate(self._classes):\n",
        "             prior = np.log(self._priors[i])\n",
        "             posterior = np.sum(np.log(self._calculate_likelihood(i, x)))\n",
        "             posterior = prior + posterior\n",
        "             posteriors.append(posterior)\n",
        "         # return the class with highest posterior probability\n",
        "         return self._classes[np.argmax(posteriors)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This code snippet performs the following tasks:\n",
        "\n",
        "1. Import necessary libraries and modules:\n",
        "   - `train_test_split` from `sklearn.model_selection`: Used to split the dataset into training and testing sets.\n",
        "   - `accuracy_score` from `sklearn.metrics`: Used to evaluate the accuracy of the predictions.\n",
        "   - `datasets` from `sklearn`: Provides synthetic datasets for testing purposes.\n",
        "   - `time`: Used to measure the execution time.\n",
        "\n",
        "2. Generate a synthetic dataset:\n",
        "   - The `make_classification` function from `datasets` is used to create a synthetic dataset.\n",
        "   - It generates 1000 samples with 20 features and 2 classes.\n",
        "   - The `random_state` parameter is set to 42 for reproducibility.\n",
        "\n",
        "3. Split the dataset into training and testing sets:\n",
        "   - The `train_test_split` function is used to split the dataset into training and testing sets.\n",
        "   - The testing set size is set to 25% of the whole dataset.\n",
        "   - The `random_state` parameter is set to 42 for reproducibility.\n",
        "\n",
        "4. Record the starting time:\n",
        "   - The `time.perf_counter()` function is used to measure the current time.\n",
        "\n",
        "5. Create and train a Gaussian Naive Bayes classifier:\n",
        "   - An instance of the `GaussianNaiveBayes` classifier is created.\n",
        "   - The `fit` method is called to train the classifier on the training data.\n",
        "\n",
        "6. Make predictions on the testing data:\n",
        "   - The `predict` method is called to make predictions on the testing data.\n",
        "\n",
        "7. Record the ending time:\n",
        "   - The `time.perf_counter()` function is used again to measure the current time.\n",
        "\n",
        "8. Print the accuracy of the predictions:\n",
        "   - The `accuracy_score` function is used to calculate the accuracy of the predicted labels compared to the true labels.\n",
        "   - The accuracy score is printed to the console.\n",
        "\n",
        "9. Print the time taken to train and predict:\n",
        "   - The difference between the ending time and starting time is calculated to measure the execution time.\n",
        "   - The execution time is printed to the console.\n",
        "\n",
        "The code demonstrates the process of training a Gaussian Naive Bayes classifier on a synthetic dataset and evaluating its accuracy on unseen data. The timing measurements provide insights into the efficiency of the training and prediction processes."
      ],
      "metadata": {
        "id": "bMTHvQ4QXz_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The provided code performs a classification task using the Naive Bayes algorithm from the scikit-learn library. Let's go through it step by step:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "import time\n",
        "```\n",
        "\n",
        "The code begins by importing necessary libraries and modules. `train_test_split` is imported from `sklearn.model_selection` for splitting the dataset into training and testing sets. `accuracy_score` from `sklearn.metrics` is imported to evaluate the accuracy of the predictions. The `datasets` module from scikit-learn is imported to generate a synthetic dataset for classification. The `time` module is imported to measure the execution time of the algorithm.\n",
        "\n",
        "```python\n",
        "X, y = datasets.make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "```\n",
        "\n",
        "The code uses the `make_classification` function from `sklearn.datasets` to generate a synthetic dataset for classification. It creates 1000 samples with 20 features and 2 classes. The `random_state` parameter is set to 42 to ensure reproducibility.\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "```\n",
        "\n",
        "The dataset is split into training and testing sets using the `train_test_split` function. 75% of the data is used for training (`X_train` and `y_train`), and 25% is used for testing (`X_test` and `y_test`). Again, the `random_state` parameter is set to 42 for reproducibility.\n",
        "\n",
        "```python\n",
        "start = time.perf_counter()\n",
        "```\n",
        "\n",
        "The current time is recorded using `time.perf_counter()` to measure the execution time of the algorithm.\n",
        "\n",
        "```python\n",
        "nb = GaussianNaiveBayes()\n",
        "nb.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "An instance of the Gaussian Naive Bayes classifier is created using `GaussianNaiveBayes()`. Note that the code provided doesn't include the import statement for `GaussianNaiveBayes`, so assuming it has been imported correctly. The classifier is then trained on the training data using the `fit` method.\n",
        "\n",
        "```python\n",
        "predictions = nb.predict(X_test)\n",
        "```\n",
        "\n",
        "The trained classifier is used to make predictions on the testing set (`X_test`) using the `predict` method.\n",
        "\n",
        "```python\n",
        "end = time.perf_counter()\n",
        "```\n",
        "\n",
        "The current time is recorded again to calculate the execution time of the algorithm.\n",
        "\n",
        "```python\n",
        "print(f\"NumPy Naive Bayes accuracy: {accuracy_score(y_test, predictions)}\")\n",
        "```\n",
        "\n",
        "The accuracy of the Naive Bayes classifier is calculated by comparing the predicted labels (`predictions`) with the true labels (`y_test`). The `accuracy_score` function is used for this calculation, and the result is printed.\n",
        "\n",
        "```python\n",
        "print(f'Finished in {round(end-start, 3)} second(s)')\n",
        "```\n",
        "\n",
        "The total execution time of the algorithm is calculated by subtracting the start time from the end time. It is then printed to the console, rounded to three decimal places.\n",
        "\n",
        "Overall, this code generates a synthetic dataset, splits it into training and testing sets, trains a Gaussian Naive Bayes classifier on the training data, makes predictions on the testing data, calculates and prints the accuracy of the classifier, and measures and prints the execution time of the algorithm. It's a simple example to demonstrate the basic usage of the Naive Bayes classifier for classification tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "bHwJyGcZVaS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "import time\n",
        "# Generate a synthetic dataset with 1000 samples, 20 features, and 2 classes\n",
        "\n",
        "X, y = datasets.make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets (75% for training, 25% for testing)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "# Record the starting time\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "# Create an instance of the Gaussian Naive Bayes classifier\n",
        "\n",
        "nb = GaussianNaiveBayes()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "\n",
        "predictions = nb.predict(X_test)\n",
        "\n",
        "# Record the ending time\n",
        "\n",
        "end = time.perf_counter()\n",
        "\n",
        "# Print the accuracy of the predictions using the testing labels\n",
        "\n",
        "print(f\"NumPy Naive Bayes accuracy: {accuracy_score(y_test, predictions)}\")\n",
        "\n",
        "# Print the time taken to train and predict in seconds\n",
        "\n",
        "print(f'Finished in {round(end-start, 3)} second(s)')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLJUXic8Xypn",
        "outputId": "0770ba66-a9ea-41ad-b152-aa202bf4bcf5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Naive Bayes accuracy: 0.796\n",
            "Finished in 0.013 second(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OYJyLdUoZNr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We use the sklearn library for Python***"
      ],
      "metadata": {
        "id": "BytOY8zTYeq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a markdown-formatted explanation of the provided code:\n",
        "\n",
        "```python\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "start = time.perf_counter()\n",
        "sk_nb = GaussianNB()\n",
        "sk_nb.fit(X_train, y_train)\n",
        "sk_predictions = sk_nb.predict(X_test)\n",
        "end = time.perf_counter()\n",
        "print(f\"scikit-learn Naive Bayes accuracy: {accuracy_score(y_test, sk_predictions)}\")\n",
        "print(f'Finished in {round(end-start, 3)} second(s)')\n",
        "```\n",
        "\n",
        "This code snippet demonstrates the usage of scikit-learn's Gaussian Naive Bayes classifier (`GaussianNB`). Let's break it down step by step:\n",
        "\n",
        "1. The code begins by importing the `GaussianNB` class from the `sklearn.naive_bayes` module. This class represents the Gaussian Naive Bayes classifier, which is a probabilistic machine learning algorithm based on Bayes' theorem.\n",
        "\n",
        "3. The `start` variable is assigned the current time using the `time.perf_counter()` function. This is used to measure the execution time of the code.\n",
        "\n",
        "4. An instance of the `GaussianNB` class is created and assigned to the variable `sk_nb`. This will be our Naive Bayes classifier.\n",
        "\n",
        "5. The `fit()` method is called on the `sk_nb` object, with `X_train` and `y_train` as its arguments. This trains the classifier on the provided training data `X_train` (features) and `y_train` (labels). The classifier learns the underlying probability distribution of the data.\n",
        "\n",
        "6. The `predict()` method is called on the trained `sk_nb` classifier, passing `X_test` as its argument. This predicts the labels for the test data `X_test` based on the learned probability distribution.\n",
        "\n",
        "7. The predicted labels are assigned to the variable `sk_predictions`.\n",
        "\n",
        "8. The `end` variable is assigned the current time using `time.perf_counter()`. This marks the end of the code execution.\n",
        "\n",
        "9. The code prints two lines of output:\n",
        "   - The first line displays the accuracy of the predictions made by the scikit-learn Naive Bayes classifier. It uses the `accuracy_score()` function, which calculates the accuracy by comparing the predicted labels (`sk_predictions`) with the true labels (`y_test`).\n",
        "   - The second line displays the total time taken to execute the code, calculated by subtracting the `start` time from the `end` time and rounding the result to three decimal places.\n",
        "\n",
        "This code is useful for classification tasks where the underlying data is assumed to follow a Gaussian distribution. It can be applied to a wide range of problems, such as spam detection, sentiment analysis, or medical diagnosis. By using scikit-learn's implementation of the Gaussian Naive Bayes classifier, developers can easily train and evaluate the model without having to implement the algorithm from scratch."
      ],
      "metadata": {
        "id": "bPfbxa-mY1vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "start = time.perf_counter()\n",
        "sk_nb = GaussianNB()\n",
        "sk_nb.fit(X_train, y_train)\n",
        "sk_predictions = sk_nb.predict(X_test)\n",
        "end = time.perf_counter()\n",
        "print(f\"scikit-learn Naive Bayes accuracy: {accuracy_score(y_test, sk_predictions)}\")\n",
        "print(f'Finished in {round(end-start, 3)} second(s)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQN_ARcbZSvQ",
        "outputId": "abfe2923-c29b-4672-c5cd-35c04eb605af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn Naive Bayes accuracy: 0.796\n",
            "Finished in 0.003 second(s)\n"
          ]
        }
      ]
    }
  ]
}